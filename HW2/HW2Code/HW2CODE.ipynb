{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions \n",
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict\n",
    "\n",
    "# read HW2 data by using spark context\n",
    "#textFile = sc.textFile(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\")\n",
    "\n",
    "# with open (\"/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\",'r',encoding = 'utf8') as file:\n",
    "#     data = csv.reader(file,delimiter = \",\")\n",
    "#     dataset = list(data)\n",
    "# print(\"dataset long:\",len(dataset))\n",
    "\n",
    "#scDataSet = sc.parallelize(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t****** Starting processing total func ******\n",
      "40602\n",
      "Done\n",
      "\n",
      "55159\n",
      "Done\n",
      "\n",
      "\t****** Processing total func Done!******\n",
      "\n",
      "\t****** Starting processing topic func ******\n",
      "19235\n",
      "Done\n",
      "\n",
      "20764\n",
      "Done\n",
      "\n",
      "15546\n",
      "Done\n",
      "\n",
      "11038\n",
      "Done\n",
      "\n",
      "24762\n",
      "Done\n",
      "\n",
      "30175\n",
      "Done\n",
      "\n",
      "22359\n",
      "Done\n",
      "\n",
      "17210\n",
      "Done\n",
      "\n",
      "\t****** Processing topic func Done!******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1 #\n",
    "# Des : In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, \n",
    "# and list the most frequent words according to the term frequency in descending order, \n",
    "# in total, per day, and per topic, respectively\n",
    "\n",
    "# input data by using sqlContext, split data by comma, and use header as dataFrame column name\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\")\n",
    "\n",
    "def regAndKeyValue(collectData):\n",
    "    # regular expression setting\n",
    "    # \"[a-zA-Z]+\"\n",
    "    # key, value count \n",
    "    wordCount = {}\n",
    "    orderResult = {}\n",
    "    Data = collectData.collect()\n",
    "    # retrivel all data from Title column\n",
    "    for i in range(len(collectData.collect())):\n",
    "        # split data by regular expression\n",
    "        ans = re.findall('[a-zA-z]+', str(Data[i][0]))\n",
    "        # key, value calculate\n",
    "        for w in ans:\n",
    "            if w not in wordCount:\n",
    "                wordCount[w] = 1\n",
    "            else:\n",
    "                wordCount[w] += 1\n",
    "    #print(wordCount)\n",
    "    orderResult = OrderedDict(sorted(wordCount.items(), key=lambda t: t[1], reverse=True))\n",
    "    #print(orderResult)\n",
    "    print(len(orderResult))\n",
    "    print(\"Done\\n\")\n",
    "\n",
    "\n",
    "# calculate Title and Headline Total key, value\n",
    "def total():\n",
    "    print(\"\\t****** Starting processing total func ******\")\n",
    "    # get column data from Titla column\n",
    "    columnData = ['Title','Headline']\n",
    "    # for c in column:\n",
    "    #     title = df.select([c]).show()\n",
    "    for c in columnData:\n",
    "        title = df.select([c])\n",
    "        regAndKeyValue(title)\n",
    "        \n",
    "    print(\"\\t****** Processing total func Done!******\\n\\n\")\n",
    "\n",
    "    \n",
    "# calculate Title and Headline key, value by using topic\n",
    "def topic():\n",
    "    print(\"\\t****** Starting processing topic func ******\")\n",
    "    topic = [\"obama\",\"economy\",\"microsoft\",\"palestine\"]\n",
    "    # get column data from Titla column\n",
    "    columnData = ['Title','Headline']\n",
    "    # retrivel columnData by four type topi\n",
    "    \n",
    "    for c in columnData:\n",
    "        for i in range(4):\n",
    "            # filter data by topic and extract column data\n",
    "            topicData = df.filter(df['Topic']==topic[i]).select([c])\n",
    "            regAndKeyValue(topicData)\n",
    "    print(\"\\t****** Processing topic func Done!******\\n\\n\")\n",
    "\n",
    "    \n",
    "# calculate Title and Headline key, value by using per day\n",
    "def day():\n",
    "    print(\"\\t****** Starting processing day func ******\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\\t****** Processing day func Done!******\\n\\n\")\n",
    "    \n",
    "    \n",
    "#total()\n",
    "#topic()\n",
    "day()\n",
    "# # Q1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q3 #\n",
    "# topic = [\"obama\",\"economy\",\"microsoft\",\"palestine\"]\n",
    "\n",
    "\n",
    "# #rdd = pd.read_csv('file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv')\n",
    "\n",
    "# # input data by using sqlContext, split data by comma, and use header as dataFrame column name\n",
    "# df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\")\n",
    "\n",
    "# # retrivel four topic\n",
    "# for i in range(4):\n",
    "#     print(\"\\t******\",topic[i],\"******\",\"\\n\")\n",
    "#     # calculate total score under select topic\n",
    "#     totalScore = df.filter(df['Topic']==topic[i]).select([sum('SentimentTitle')]).collect()\n",
    "#     # calculate topic number\n",
    "#     topicCount = df.filter(df['Topic']==topic[i]).count()\n",
    "#     print(topic[i]+\" TotalScore :\",totalScore[0][0])\n",
    "#     print(topic[i]+\" AveScore :\",totalScore[0][0]/topicCount,\"\\n\")\n",
    "#     print(\"\\t******\",topic[i],\"******\",\"\\n\\n\")\n",
    "    \n",
    "# # Q3 #\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q2 Done#\n",
    "\n",
    "# Facebook = [\"Facebook_Economy\",\"Facebook_Microsoft\",\"Facebook_Obama\",\"Facebook_Palestine\"]\n",
    "# Google = [\"GooglePlus_Economy\",\"GooglePlus_Microsoft\",\"GooglePlus_Palestine\"]\n",
    "# Linked = [\"LinkedIn_Economy\",\"LinkedIn_Microsoft\",\"LinkedIn_Obama\",\"LinkedIn_Palestine\"]\n",
    "\n",
    "# # processing Q2 request\n",
    "# def process(file):\n",
    "#     for f in file:\n",
    "#         # input data from csv by using spark context\n",
    "#         rdd = sc.textFile(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/\"+f+\".csv\")\n",
    "#         # remove header\n",
    "#         header = rdd.first()\n",
    "#         hrdd = rdd.filter(lambda x:x!= header)\n",
    "        \n",
    "#         # retrivel all dataset, each time calculate one row\n",
    "#         for i in hrdd.take(rdd.count()):\n",
    "#             ID =0  # for remove IDLink\n",
    "#             perDaySum = 0  # for sum score\n",
    "#             # split row data by split comma\n",
    "#             for j in i.split(\",\"):\n",
    "#                 if ID !=0:\n",
    "#                     perDaySum +=float(j)\n",
    "#                 else:\n",
    "#                     ID = float(j)\n",
    "#             perDaySum = perDaySum - ID\n",
    "#             #print(\"AveByHour :\",perDaySum/48)\n",
    "#             #print(\"AveByDay :\",perDaySum/2)\n",
    "#         print(\"Finish\")\n",
    "       \n",
    "        \n",
    "# #process(Facebook)\n",
    "# #process(Google)\n",
    "# #process(Linked)\n",
    "\n",
    "\n",
    "# # Q2 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
