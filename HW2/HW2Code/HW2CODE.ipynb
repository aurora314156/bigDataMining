{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions \n",
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t****** Starting processing total func ******\n",
      "\t****** Processing total func Done!******\n",
      "\n",
      "\n",
      "\t****** Starting processing topic func ******\n",
      "\t****** Processing topic func Done!******\n",
      "\n",
      "\n",
      "\t****** Starting processing day func ******\n",
      "\n",
      "\t****** Processing day func Done!******\n",
      "\n",
      "\n",
      "\n",
      "Q1 done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1 #\n",
    "\n",
    "# Des : In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, \n",
    "# and list the most frequent words according to the term frequency in descending order, \n",
    "# in total, per day, and per topic, respectively\n",
    "\n",
    "# input data by using sqlContext, split data by comma, and use header as dataFrame column name\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\")\n",
    "\n",
    "\n",
    "def regAndKeyValue(collectData):\n",
    "    # regular expression setting\n",
    "    # \"[a-zA-Z]+\"\n",
    "    # key, value count \n",
    "    wordCount = {}\n",
    "    orderResult = {}\n",
    "    Data = collectData.collect()\n",
    "    # retrivel all data from Title column\n",
    "    for i in range(len(collectData.collect())):\n",
    "        # split data by regular expression\n",
    "        ans = re.findall('[a-zA-z]+', str(Data[i][0]))\n",
    "        # key, value calculate\n",
    "        for w in ans:\n",
    "            if w not in wordCount:\n",
    "                wordCount[w] = 1\n",
    "            else:\n",
    "                wordCount[w] += 1\n",
    "    #print(wordCount)\n",
    "    orderResult = OrderedDict(sorted(wordCount.items(), key=lambda t: t[1], reverse=True))\n",
    "    return orderResult\n",
    "    #print(orderResult)\n",
    "    print(len(orderResult))\n",
    "    print(\"Done\\n\")\n",
    "\n",
    "\n",
    "# calculate Title and Headline Total key, value\n",
    "def total():\n",
    "    \n",
    "    print(\"\\t****** Starting processing total func ******\")\n",
    "    \n",
    "    # get column data from Titla column\n",
    "    columnData = ['Title','Headline']\n",
    "    \n",
    "    for c in columnData:\n",
    "        # write result to txt\n",
    "        filename = \"Q1_total_\" + c + \".txt\"\n",
    "        with open(filename, 'a') as out: \n",
    "            title = df.select([c])\n",
    "            result = regAndKeyValue(title)\n",
    "            out.write(json.dumps(result))\n",
    "\n",
    "    print(\"\\t****** Processing total func Done!******\\n\\n\")\n",
    "\n",
    "    \n",
    "# calculate Title and Headline key, value by using topic\n",
    "def topic():\n",
    "    \n",
    "    print(\"\\t****** Starting processing topic func ******\")\n",
    "    \n",
    "    topic = [\"obama\",\"economy\",\"microsoft\",\"palestine\"]\n",
    "    # get column data from Titla column\n",
    "    columnData = ['Title','Headline']\n",
    "    # retrivel columnData by four type topic\n",
    "    for c in columnData:\n",
    "        for i in range(4):\n",
    "            # write result to txt\n",
    "            filename = \"Q1_perTopic_\" + c + \"_\" + topic[i] + \".txt\"\n",
    "            with open(filename, 'a') as out:\n",
    "                # filter data by topic and extract column data\n",
    "                topicData = df.filter(df['Topic']==topic[i]).select([c])\n",
    "                result = regAndKeyValue(topicData)\n",
    "                out.write(json.dumps(result))\n",
    "    print(\"\\t****** Processing topic func Done!******\\n\\n\")\n",
    "\n",
    "    \n",
    "# calculate Title and Headline key, value by using PublishDate\n",
    "def publishDate():\n",
    "    \n",
    "    print(\"\\t****** Starting processing day func ******\\n\")\n",
    "    # get column data from Titla column\n",
    "    columnData = ['Title','Headline']\n",
    "    date = df.select(['PublishDate']).collect()\n",
    "    flag = \"\"\n",
    "    wordCount = {}\n",
    "    orderResult = {}\n",
    "    # retrivel columnData by publishDate\n",
    "    for c in columnData:\n",
    "        # get column data\n",
    "        data = df.select([c]).collect()\n",
    "        # retrivel publishDate column\n",
    "        for d in range(len(data)):\n",
    "            # split publishData string by \" \"\n",
    "            day = date[d][0].split(\" \")\n",
    "            # avoid error input\n",
    "            if len(day)==2:\n",
    "                # split data by regular expression\n",
    "                ans = re.findall('[a-zA-z]+', str(data[d][0]))\n",
    "                # key, value calculate\n",
    "                for w in ans:\n",
    "                    if w not in wordCount:\n",
    "                        wordCount[w] = 1\n",
    "                    else:\n",
    "                        wordCount[w] += 1            \n",
    "                # when new date apeear then initital count dict\n",
    "                if flag != day[0]:\n",
    "                    # sorting result\n",
    "                    orderResult = OrderedDict(sorted(wordCount.items(), key=lambda t: t[1], reverse=True))\n",
    "                    flag = day[0]\n",
    "                    wordCount = {}\n",
    "                    # write result to txt\n",
    "                    filename = \"Q1_perDay_\" + c + \".txt\"\n",
    "                    with open(filename, 'a') as out:\n",
    "                        out.write(flag + \"\\n\" + json.dumps(orderResult) + \"\\n\")\n",
    "                    orderResult = {}\n",
    "                    \n",
    "            #print(c,count,\" processing done!\\n\")\n",
    "                    \n",
    "    print(\"\\t****** Processing day func Done!******\\n\\n\")\n",
    "    \n",
    "    \n",
    "total()\n",
    "topic()\n",
    "publishDate()\n",
    "\n",
    "print(\"\\nQ1 done!\\n\")\n",
    "\n",
    "# # Q1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing Facebook_Economy\n",
      "Start processing Facebook_Microsoft\n",
      "Start processing Facebook_Obama\n",
      "Start processing Facebook_Palestine\n",
      "Processing Facebook_Palestine done !\n",
      "Start processing GooglePlus_Economy\n",
      "Start processing GooglePlus_Microsoft\n",
      "Start processing GooglePlus_Palestine\n",
      "Processing GooglePlus_Palestine done !\n",
      "Start processing LinkedIn_Economy\n",
      "Start processing LinkedIn_Microsoft\n",
      "Start processing LinkedIn_Obama\n",
      "Start processing LinkedIn_Palestine\n",
      "Processing LinkedIn_Palestine done !\n",
      "\n",
      "Q2 done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q2 Done#\n",
    "\n",
    "Facebook = [\"Facebook_Economy\",\"Facebook_Microsoft\",\"Facebook_Obama\",\"Facebook_Palestine\"]\n",
    "Google = [\"GooglePlus_Economy\",\"GooglePlus_Microsoft\",\"GooglePlus_Palestine\"]\n",
    "Linked = [\"LinkedIn_Economy\",\"LinkedIn_Microsoft\",\"LinkedIn_Obama\",\"LinkedIn_Palestine\"]\n",
    "\n",
    "# processing Q2 request\n",
    "def process(file):\n",
    "    for f in file:\n",
    "        print(\"Start processing \"+ f)\n",
    "        # input data from csv by using spark context\n",
    "        rdd = sc.textFile(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/\"+f+\".csv\")\n",
    "        # remove header\n",
    "        header = rdd.first()\n",
    "        hrdd = rdd.filter(lambda x:x!= header)\n",
    "        \n",
    "        # retrivel all dataset, each time calculate one row\n",
    "        for i in hrdd.take(rdd.count()):\n",
    "            ID =0  # for remove IDLink\n",
    "            perDaySum = 0  # for sum score\n",
    "            # split row data by split comma\n",
    "            for j in i.split(\",\"):\n",
    "                if ID !=0:\n",
    "                    perDaySum +=float(j)\n",
    "                else:\n",
    "                    ID = float(j)\n",
    "                    \n",
    "            perDaySum = perDaySum - ID\n",
    "            \n",
    "        # write result to txt\n",
    "        filename = \"Q2_calAvePopularity_\" + f + \".txt\"\n",
    "        with open(filename, 'a') as out:\n",
    "            out.write(\"aveByDay: \" + str(perDaySum/2) + \"\\n\" + \"aveByHour: \" + str(perDaySum/48))\n",
    "            \n",
    "    print(\"Processing \" + f + \" done !\")\n",
    "        \n",
    "process(Facebook)\n",
    "process(Google)\n",
    "process(Linked)\n",
    "\n",
    "print(\"\\nQ2 done!\\n\")\n",
    "\n",
    "# Q2 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t****** obama ****** \n",
      "\n",
      "Start collect!\n",
      "Collect done!\n",
      "\t****** obama ******\n",
      "\n",
      "\n",
      "\t****** economy ****** \n",
      "\n",
      "Start collect!\n",
      "Collect done!\n",
      "\t****** economy ******\n",
      "\n",
      "\n",
      "\t****** microsoft ****** \n",
      "\n",
      "Start collect!\n",
      "Collect done!\n",
      "\t****** microsoft ******\n",
      "\n",
      "\n",
      "\t****** palestine ****** \n",
      "\n",
      "Start collect!\n",
      "Collect done!\n",
      "\t****** palestine ******\n",
      "\n",
      "\n",
      "\n",
      "Q3 done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q3 #\n",
    "topic = [\"obama\",\"economy\",\"microsoft\",\"palestine\"]\n",
    "\n",
    "\n",
    "# input data by using sqlContext, split data by comma, and use header as dataFrame column name\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\")\n",
    "\n",
    "\n",
    "def Q3():\n",
    "    # retrivel four topic\n",
    "    for i in range(4):\n",
    "        print(\"\\t******\",topic[i],\"******\",\"\\n\")\n",
    "        # calculate total score under select topic\n",
    "        print(\"Start collect!\")\n",
    "        ans = df.filter(df['Topic']==topic[i]).select(['SentimentTitle']).collect()\n",
    "        print(\"Collect done!\")\n",
    "        totalScore = 0\n",
    "        for j in range(df.filter(df['Topic']==topic[i]).count()):\n",
    "            totalScore += float(ans[j][0])\n",
    "        # calculate topic number\n",
    "        topicCount = df.filter(df['Topic']==topic[i]).count()\n",
    "        # write result to txt\n",
    "        filename = \"Q3_calSentimentScore_\" + topic[i] + \".txt\"\n",
    "        with open(filename, 'a') as out:\n",
    "            out.write(\"aveScore: \" + str(totalScore/topicCount) + \"\\nTotalScore: \" + str(totalScore))\n",
    "        print(\"\\t******\",topic[i],\"******\\n\\n\")\n",
    "\n",
    "Q3()\n",
    "        \n",
    "# Q3 #\n",
    "\n",
    "    \n",
    "print(\"\\nQ3 done!\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title start processing task  0\n",
      "Title task  0  processing done!\n",
      "Title start processing task  1\n",
      "Title task  1  processing done!\n",
      "Title start processing task  2\n",
      "Title task  2  processing done!\n",
      "Title start processing task  3\n",
      "Title task  3  processing done!\n",
      "Headline start processing task  0\n",
      "Headline task  0  processing done!\n",
      "Headline start processing task  1\n",
      "Headline task  1  processing done!\n",
      "Headline start processing task  2\n",
      "Headline task  2  processing done!\n",
      "Headline start processing task  3\n",
      "Headline task  3  processing done!\n",
      "\n",
      "Q4 done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4 #\n",
    "\n",
    "# input data by using sqlContext, split data by comma, and use header as dataFrame column name\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\")\n",
    "\n",
    "# calculate termFrequency            \n",
    "def termFrequency(Data):\n",
    "    # save term frequency and order result\n",
    "    wordCount = {}\n",
    "    orderResult = {}\n",
    "    # retrivel filter data\n",
    "    for i in range(len(Data)):\n",
    "        # split data by regular expression\n",
    "        rowData = re.findall('[a-zA-z]+', str(Data[i][0]))\n",
    "        #key, value calculate\n",
    "        for w in rowData:\n",
    "            if w not in wordCount:\n",
    "                wordCount[w] = 1\n",
    "            else:\n",
    "                wordCount[w] += 1\n",
    "    # order term frequency\n",
    "    orderResult = OrderedDict(sorted(wordCount.items(), key=lambda t: t[1], reverse=True))\n",
    "    return orderResult\n",
    "    \n",
    "# calculate co-occurrence\n",
    "def coOccurrence(termItem, Data, filename):\n",
    "    # initial co-occurrence matrix by using numpy\n",
    "    coMatrix = np.zeros((100,100),int)\n",
    "    # retrivel filter data\n",
    "    for i in range(len(Data)):\n",
    "        # split data by regular expression\n",
    "        rowData = re.findall('[a-zA-z]+', str(Data[i][0]))\n",
    "        # retrivel order term frequency list\n",
    "        for t in range(100):\n",
    "            # retrivel each row data\n",
    "            if termItem[t] in rowData:\n",
    "                # retirvel other keyword from order list\n",
    "                for w in range(100):\n",
    "                    # avoid compare to keyword itself, and calculate two keyword appear in rowData\n",
    "                    if termItem[w] in rowData and termItem[w] is not termItem[t]:\n",
    "                        coMatrix[t][w] += 1\n",
    "    # write result to txt\n",
    "    with open(filename, 'a') as out:\n",
    "        out.write(\"   \")\n",
    "        for j in range(100):\n",
    "            out.write(termItem[j] + \" \")\n",
    "        out.write(\"\\n\")\n",
    "        for i in range(100):\n",
    "            out.write(termItem[i] + \" \")\n",
    "            for k in range(100):\n",
    "                out.write(str(coMatrix[i][k]) + \" \")\n",
    "            out.write(\"\\n\")\n",
    "    \n",
    "# get topic\n",
    "topic = [\"obama\",\"economy\",\"microsoft\",\"palestine\"]\n",
    "# get columnData\n",
    "columnData = ['Title','Headline']\n",
    "\n",
    "# main func\n",
    "def main():\n",
    "    for c in columnData:\n",
    "        for t in range(4):\n",
    "            print(c, \"start processing task \",t)\n",
    "            # filter data by using topic\n",
    "            Data = df.filter(df['Topic']==topic[t]).select([c]).collect()\n",
    "            # get ordered term frequency <key,value> \n",
    "            orderTermFrequency = termFrequency(Data)\n",
    "            # transfer OrderedDict to list\n",
    "            termItem = list(orderTermFrequency)\n",
    "            # write result to txt\n",
    "            filename = \"Q4_co-occurence_\" + c + \"_\" + topic[t] + \".txt\"\n",
    "            # process co-occurrence matrix\n",
    "            coOccurrence(termItem, Data, filename)\n",
    "            \n",
    "            print(c,\"task \",t,\" processing done!\")\n",
    "            \n",
    "# program start point\n",
    "main()\n",
    "\n",
    "print(\"\\nQ4 done!\\n\")\n",
    "\n",
    "# Q4 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
