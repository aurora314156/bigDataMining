{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions \n",
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1 done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Q1 #\n",
    "\n",
    "# # Des : In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, \n",
    "# # and list the most frequent words according to the term frequency in descending order, \n",
    "# # in total, per day, and per topic, respectively\n",
    "\n",
    "# # input data by using sqlContext, split data by comma, and use header as dataFrame column name\n",
    "# df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\")\n",
    "\n",
    "\n",
    "# def regAndKeyValue(collectData):\n",
    "#     # regular expression setting\n",
    "#     # \"[a-zA-Z]+\"\n",
    "#     # key, value count \n",
    "#     wordCount = {}\n",
    "#     orderResult = {}\n",
    "#     Data = collectData.collect()\n",
    "#     # retrivel all data from Title column\n",
    "#     for i in range(len(collectData.collect())):\n",
    "#         # split data by regular expression\n",
    "#         ans = re.findall('[a-zA-z]+', str(Data[i][0]))\n",
    "#         # key, value calculate\n",
    "#         for w in ans:\n",
    "#             if w not in wordCount:\n",
    "#                 wordCount[w] = 1\n",
    "#             else:\n",
    "#                 wordCount[w] += 1\n",
    "#     #print(wordCount)\n",
    "#     orderResult = OrderedDict(sorted(wordCount.items(), key=lambda t: t[1], reverse=True))\n",
    "#     #print(orderResult)\n",
    "#     print(len(orderResult))\n",
    "#     print(\"Done\\n\")\n",
    "\n",
    "\n",
    "# # calculate Title and Headline Total key, value\n",
    "# def total():\n",
    "    \n",
    "#     print(\"\\t****** Starting processing total func ******\")\n",
    "    \n",
    "#     # get column data from Titla column\n",
    "#     columnData = ['Title','Headline']\n",
    "    \n",
    "#     for c in columnData:\n",
    "#         title = df.select([c])\n",
    "#         regAndKeyValue(title)\n",
    "        \n",
    "#     print(\"\\t****** Processing total func Done!******\\n\\n\")\n",
    "\n",
    "    \n",
    "# # calculate Title and Headline key, value by using topic\n",
    "# def topic():\n",
    "    \n",
    "#     print(\"\\t****** Starting processing topic func ******\")\n",
    "    \n",
    "#     topic = [\"obama\",\"economy\",\"microsoft\",\"palestine\"]\n",
    "#     # get column data from Titla column\n",
    "#     columnData = ['Title','Headline']\n",
    "#     # retrivel columnData by four type topic\n",
    "#     for c in columnData:\n",
    "#         for i in range(4):\n",
    "#             # filter data by topic and extract column data\n",
    "#             topicData = df.filter(df['Topic']==topic[i]).select([c])\n",
    "#             regAndKeyValue(topicData)\n",
    "            \n",
    "#     print(\"\\t****** Processing topic func Done!******\\n\\n\")\n",
    "\n",
    "    \n",
    "# # calculate Title and Headline key, value by using PublishDate\n",
    "# def publishDate():\n",
    "    \n",
    "#     print(\"\\t****** Starting processing day func ******\\n\")\n",
    "#     # get column data from Titla column\n",
    "#     columnData = ['Title','Headline']\n",
    "#     date = df.select(['PublishDate']).collect()\n",
    "#     flag = \"\"\n",
    "#     wordCount = {}\n",
    "#     orderResult = {}\n",
    "#     # retrivel columnData by publishDate\n",
    "#     for c in columnData:\n",
    "#         # get column data\n",
    "#         data = df.select([c]).collect()\n",
    "#         # retrivel publishDate column\n",
    "#         for d in range(len(data)):\n",
    "#             # split publishData string by \" \"\n",
    "#             day = date[d][0].split(\" \")\n",
    "#             # avoid error input\n",
    "#             if len(day)==2:\n",
    "#                 # split data by regular expression\n",
    "#                 ans = re.findall('[a-zA-z]+', str(data[d][0]))\n",
    "#                 # key, value calculate\n",
    "#                 for w in ans:\n",
    "#                     if w not in wordCount:\n",
    "#                         wordCount[w] = 1\n",
    "#                     else:\n",
    "#                         wordCount[w] += 1            \n",
    "#                 # when new date apeear then initital count dict\n",
    "#                 if flag != day[0]:\n",
    "#                     # sorting result\n",
    "#                     orderResult = OrderedDict(sorted(wordCount.items(), key=lambda t: t[1], reverse=True))\n",
    "#                     #print(len(orderResult))\n",
    "#                     wordCount = {}\n",
    "#                     orderResult = {}\n",
    "#                     flag = day[0]\n",
    "#         #print(c,\" processing done!\\n\")\n",
    "                    \n",
    "#     print(\"\\t****** Processing day func Done!******\\n\\n\")\n",
    "    \n",
    "    \n",
    "# #total()\n",
    "# #topic()\n",
    "# publishDate()\n",
    "\n",
    "print(\"\\nQ1 done!\\n\")\n",
    "\n",
    "# # Q1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q2 done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Q2 Done#\n",
    "\n",
    "# Facebook = [\"Facebook_Economy\",\"Facebook_Microsoft\",\"Facebook_Obama\",\"Facebook_Palestine\"]\n",
    "# Google = [\"GooglePlus_Economy\",\"GooglePlus_Microsoft\",\"GooglePlus_Palestine\"]\n",
    "# Linked = [\"LinkedIn_Economy\",\"LinkedIn_Microsoft\",\"LinkedIn_Obama\",\"LinkedIn_Palestine\"]\n",
    "\n",
    "# # processing Q2 request\n",
    "# def process(file):\n",
    "#     for f in file:\n",
    "#         # input data from csv by using spark context\n",
    "#         rdd = sc.textFile(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/\"+f+\".csv\")\n",
    "#         # remove header\n",
    "#         header = rdd.first()\n",
    "#         hrdd = rdd.filter(lambda x:x!= header)\n",
    "        \n",
    "#         # retrivel all dataset, each time calculate one row\n",
    "#         for i in hrdd.take(rdd.count()):\n",
    "#             ID =0  # for remove IDLink\n",
    "#             perDaySum = 0  # for sum score\n",
    "#             # split row data by split comma\n",
    "#             for j in i.split(\",\"):\n",
    "#                 if ID !=0:\n",
    "#                     perDaySum +=float(j)\n",
    "#                 else:\n",
    "#                     ID = float(j)\n",
    "#             perDaySum = perDaySum - ID\n",
    "#             #print(\"AveByHour :\",perDaySum/48)\n",
    "#             #print(\"AveByDay :\",perDaySum/2)\n",
    "#         print(\"Finish\")\n",
    "       \n",
    "        \n",
    "# #process(Facebook)\n",
    "# #process(Google)\n",
    "# #process(Linked)\n",
    "\n",
    "print(\"\\nQ2 done!\\n\")\n",
    "\n",
    "# # Q2 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q3 done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Q3 #\n",
    "# topic = [\"obama\",\"economy\",\"microsoft\",\"palestine\"]\n",
    "\n",
    "\n",
    "# #rdd = pd.read_csv('file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv')\n",
    "\n",
    "# # input data by using sqlContext, split data by comma, and use header as dataFrame column name\n",
    "# df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\")\n",
    "\n",
    "# # retrivel four topic\n",
    "# for i in range(4):\n",
    "#     print(\"\\t******\",topic[i],\"******\",\"\\n\")\n",
    "#     # calculate total score under select topic\n",
    "#     totalScore = df.filter(df['Topic']==topic[i]).select([sum('SentimentTitle')]).collect()\n",
    "#     # calculate topic number\n",
    "#     topicCount = df.filter(df['Topic']==topic[i]).count()\n",
    "#     print(topic[i]+\" TotalScore :\",totalScore[0][0])\n",
    "#     print(topic[i]+\" AveScore :\",totalScore[0][0]/topicCount,\"\\n\")\n",
    "#     print(\"\\t******\",topic[i],\"******\",\"\\n\\n\")\n",
    "    \n",
    "# # Q3 #\n",
    "\n",
    "    \n",
    "print(\"\\nQ3 done!\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title start processing task  0\n",
      "[[   0 7655 6760 ...  225  210  217]\n",
      " [7655    0 1501 ...   76   48   42]\n",
      " [6760 1501    0 ...   47   53   48]\n",
      " ...\n",
      " [ 225   76   47 ...    0    1    6]\n",
      " [ 210   48   53 ...    1    0    3]\n",
      " [ 217   42   48 ...    6    3    0]]\n",
      "Title task  0  processing done!\n",
      "Title start processing task  1\n",
      "[[   0 4398   19 ...  158  144  194]\n",
      " [4398    0 1399 ...   57   70  130]\n",
      " [  19 1399    0 ...   44   40   26]\n",
      " ...\n",
      " [ 158   57   44 ...    0    0   12]\n",
      " [ 144   70   40 ...    0    0    0]\n",
      " [ 194  130   26 ...   12    0    0]]\n",
      "Title task  1  processing done!\n",
      "Title start processing task  2\n",
      "[[   0 4867 4104 ...  206  190  197]\n",
      " [4867    0  868 ...   59   41   38]\n",
      " [4104  868    0 ...   95   59   60]\n",
      " ...\n",
      " [ 206   59   95 ...    0    0    0]\n",
      " [ 190   41   59 ...    0    0    4]\n",
      " [ 197   38   60 ...    0    4    0]]\n",
      "Title task  2  processing done!\n",
      "Title start processing task  3\n",
      "[[   0 1006  987 ...   48   39   33]\n",
      " [1006    0  348 ...    9    1   15]\n",
      " [ 987  348    0 ...   15   30   22]\n",
      " ...\n",
      " [  48    9   15 ...    0    0    0]\n",
      " [  39    1   30 ...    0    0    0]\n",
      " [  33   15   22 ...    0    0    0]]\n",
      "Title task  3  processing done!\n",
      "Headline start processing task  0\n",
      "[[    0 18798 11627 ...   624   564   520]\n",
      " [18798     0 14551 ...   736   704   667]\n",
      " [11627 14551     0 ...   502   444   455]\n",
      " ...\n",
      " [  624   736   502 ...     0    36    26]\n",
      " [  564   704   444 ...    36     0    33]\n",
      " [  520   667   455 ...    26    33     0]]\n",
      "Headline task  0  processing done!\n",
      "Headline start processing task  1\n",
      "[[    0 18350 14347 ...   728   755   710]\n",
      " [18350     0 10658 ...   583   697   703]\n",
      " [14347 10658     0 ...   443   478   406]\n",
      " ...\n",
      " [  728   583   443 ...     0     9    29]\n",
      " [  755   697   478 ...     9     0    30]\n",
      " [  710   703   406 ...    29    30     0]]\n",
      "Headline task  1  processing done!\n",
      "Headline start processing task  2\n",
      "[[    0 13395  7958 ...   436   370   453]\n",
      " [13395     0 10800 ...   579   525   573]\n",
      " [ 7958 10800     0 ...   372   375   401]\n",
      " ...\n",
      " [  436   579   372 ...     0    32    15]\n",
      " [  370   525   375 ...    32     0    17]\n",
      " [  453   573   401 ...    15    17     0]]\n",
      "Headline task  2  processing done!\n",
      "Headline start processing task  3\n",
      "[[   0 3948 2819 ...  142  176  145]\n",
      " [3948    0 2291 ...  115  150   93]\n",
      " [2819 2291    0 ...   80   87   99]\n",
      " ...\n",
      " [ 142  115   80 ...    0   10    2]\n",
      " [ 176  150   87 ...   10    0    2]\n",
      " [ 145   93   99 ...    2    2    0]]\n",
      "Headline task  3  processing done!\n",
      "\n",
      "Q1 done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4 #\n",
    "\n",
    "# input data by using sqlContext, split data by comma, and use header as dataFrame column name\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load(\"file:/home/ethan/pythonwork/ipynotebook/HW2/HW2Data/News_Final.csv\")\n",
    "\n",
    "# get topic\n",
    "topic = [\"obama\",\"economy\",\"microsoft\",\"palestine\"]\n",
    "# get columnData\n",
    "columnData = ['Title','Headline']\n",
    "\n",
    "# calculate co-occurrence\n",
    "def coOccurrence(termItem, Data):\n",
    "    # initial co-occurrence matrix by using numpy\n",
    "    coMatrix = np.zeros((100,100),int)\n",
    "    # retrivel filter data\n",
    "    for i in range(len(Data)):\n",
    "        # split data by regular expression\n",
    "        rowData = re.findall('[a-zA-z]+', str(Data[i][0]))\n",
    "        # retrivel order term frequency list\n",
    "        for t in range(100):\n",
    "            # retrivel each row data\n",
    "            if termItem[t] in rowData:\n",
    "                # retirvel other keyword from order list\n",
    "                for w in range(100):\n",
    "                    # avoid compare to keyword itself, and calculate two keyword appear in rowData\n",
    "                    if termItem[w] in rowData and termItem[w] is not termItem[t]:\n",
    "                        coMatrix[t][w] += 1\n",
    "    print(coMatrix)\n",
    "    \n",
    "    \n",
    "# calculate termFrequency            \n",
    "def termFrequency(Data):\n",
    "    # save term frequency and order result\n",
    "    wordCount = {}\n",
    "    orderResult = {}\n",
    "    # retrivel filter data\n",
    "    #for i in range(len(Data))\n",
    "    for i in range(len(Data)):\n",
    "        # split data by regular expression\n",
    "        rowData = re.findall('[a-zA-z]+', str(Data[i][0]))\n",
    "        #key, value calculate\n",
    "        for w in rowData:\n",
    "            if w not in wordCount:\n",
    "                wordCount[w] = 1\n",
    "            else:\n",
    "                wordCount[w] += 1\n",
    "    # order term frequency\n",
    "    orderResult = OrderedDict(sorted(wordCount.items(), key=lambda t: t[1], reverse=True))\n",
    "    return orderResult\n",
    "    # print(orderResult)\n",
    "\n",
    "    \n",
    "# main func\n",
    "def main():\n",
    "    for c in columnData:\n",
    "        for t in range(4):\n",
    "            print(c, \"start processing task \",t)\n",
    "            # filter data by using topic\n",
    "            Data = df.filter(df['Topic']==topic[t]).select([c]).collect()\n",
    "            #termFrequency(Data)\n",
    "            orderTermFrequency = termFrequency(Data)\n",
    "            # transfer OrderedDict to list\n",
    "            termItem = list(orderTermFrequency)\n",
    "            # process co-occurrence matrix\n",
    "            coOccurrence(termItem, Data)\n",
    "            \n",
    "            print(c,\"task \",t,\" processing done!\")\n",
    "            \n",
    "# program start point\n",
    "main()\n",
    "\n",
    "        \n",
    "print(\"\\nQ1 done!\\n\")\n",
    "\n",
    "\n",
    "# Q4 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
